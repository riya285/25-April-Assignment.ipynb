{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f6838-db22-4785-b28f-f958250b2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Eigenvalues and Eigenvectors are fundamental concepts in linear algebra and matrix theory. Eigenvalues are the scalars by which the linear transformation of a vector scales it, while Eigenvectors are the vectors that get scaled by the corresponding eigenvalue. In the context of the Eigen-Decomposition approach, the Eigenvectors and Eigenvalues form the matrix basis of the original matrix.\n",
    "\n",
    "Q2. Eigen decomposition, also known as spectral decomposition, is a process that transforms a matrix into its eigenvalues and eigenvectors. In the context of linear algebra, the spectral theorem extends the concept of eigen decomposition to compact, self-adjoint linear operators.\n",
    "\n",
    "Q3. For a square matrix to be diagonalizable using the Eigen-Decomposition approach, it must have n linearly independent eigenvectors, where n is the number of rows or columns in the matrix. This is because a matrix is diagonalizable if and only if its minimal polynomial splits into linear factors.\n",
    "\n",
    "Q4. The spectral theorem states that any square complex matrix can be decomposed into the sum of its eigenvalue-multiplied eigenvectors and its own conjugate transpose. In other words, it guarantees that any square matrix is diagonalizable using the Eigen-Decomposition approach.\n",
    "\n",
    "Q5. To find the eigenvalues of a matrix, we need to solve the characteristic equation, which is given by det(A - λI) = 0, where A is the matrix, λ is the eigenvalue, and I is the identity matrix.\n",
    "\n",
    "Q6. Eigenvectors are the vectors that get scaled by the corresponding eigenvalue, while Eigenvalues are the scalars by which the linear transformation of a vector scales it. In other words, if a matrix A is multiplied by a vector v, the resulting vector Av is the eigenvector scaled by the eigenvalue λ.\n",
    "\n",
    "Q7. The geometric interpretation of eigenvectors and eigenvalues can be understood by visualizing the eigenvectors as the axes of maximum and minimum variance of the data and the eigenvalues as the corresponding magnitudes.\n",
    "\n",
    "Q8. Real-world applications of eigen decomposition include image compression, principal component analysis (PCA), and machine learning algorithms like Support Vector Machines (SVMs).\n",
    "\n",
    "Q9. No, a matrix can have at most n distinct eigenvectors and eigenvalues, where n is the number of rows or columns in the matrix. However, a matrix can have multiple sets of linearly independent eigenvectors and corresponding eigenvalues, leading to repeated eigenvalues and degenerate eigenvectors.\n",
    "\n",
    "Q10. The Eigen-Decomposition approach is useful in data analysis and machine learning for tasks such as dimensionality reduction, anomaly detection, and optimizing algorithm performance. In particular, techniques like PCA, which relies on the Eigen-Decomposition of the covariance matrix of the data, and SVMs, which use a kernel trick that implicitly applies the Eigen-Decomposition to the data, are prime examples of the applicability of this approach.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
